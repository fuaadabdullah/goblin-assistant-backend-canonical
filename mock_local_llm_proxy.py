#!/usr/bin/env python3
"""
Mock Local LLM Proxy for testing backend integration
without requiring actual LLM models or disk space.
"""

import json
import time
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Mock Local LLM Proxy", version="1.0.0")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: list[ChatMessage]
    temperature: float = 0.7
    max_tokens: int = 100

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "mock-local-llm-proxy"}

@app.get("/v1/models")
async def list_models():
    """List available models"""
    return {
        "object": "list",
        "data": [
            {
                "id": "llama2:7b",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "ollama"
            },
            {
                "id": "mistral:7b",
                "object": "model", 
                "created": int(time.time()),
                "owned_by": "ollama"
            }
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completion(request: ChatCompletionRequest, req: Request):
    """Mock chat completion endpoint"""
    # Check API key
    auth_header = req.headers.get("authorization", "")
    if not auth_header.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Invalid API key")
    
    api_key = auth_header.replace("Bearer ", "")
    if api_key != "dev-test-key-12345":
        raise HTTPException(status_code=401, detail="Invalid API key")
    
    # Get the last user message
    user_messages = [msg for msg in request.messages if msg.role == "user"]
    if not user_messages:
        raise HTTPException(status_code=400, detail="No user message found")
    
    user_content = user_messages[-1].content
    
    # Generate mock response based on input
    if "hello" in user_content.lower():
        mock_response = "Hello! I'm a mock local LLM running on your development machine. This response is generated to test the backend integration."
    elif "test" in user_content.lower():
        mock_response = "Test successful! The local LLM proxy is working correctly. This confirms that the routing system can successfully connect to local models."
    else:
        mock_response = f"I received your message: '{user_content}'. This is a mock response from the local LLM proxy. In production, this would be generated by Ollama or llama.cpp running on your Kamatera server."
    
    return {
        "id": f"chatcmpl-mock-{int(time.time())}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": request.model,
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": mock_response
            },
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": len(user_content.split()),
            "completion_tokens": len(mock_response.split()),
            "total_tokens": len(user_content.split()) + len(mock_response.split())
        }
    }

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "service": "Mock Local LLM Proxy",
        "version": "1.0.0",
        "endpoints": ["GET /health", "GET /v1/models", "POST /v1/chat/completions"],
        "note": "This is a mock service for testing backend integration"
    }

if __name__ == "__main__":
    print("ðŸš€ Starting Mock Local LLM Proxy on http://localhost:8002")
    print("Test with: curl http://localhost:8002/health")
    uvicorn.run(app, host="0.0.0.0", port=8002, log_level="info")
