"""
Master Test Suite Runner
Runs all 4 advanced test suites in sequence:
1. Model comparison
2. RAG pipeline coherence
3. Latency stress testing
4. Safety triage

Generates comprehensive final report.
"""

import asyncio
import subprocess
import time
import json
from pathlib import Path
from datetime import datetime


def run_test_script(script_name: str, description: str):
    """Run a test script and capture output."""
    print(f"\n{'=' * 80}")
    print(f"RUNNING: {description}")
    print(f"Script: {script_name}")
    print(f"{'=' * 80}\n")

    start_time = time.time()

    try:
        result = subprocess.run(
            ["python", script_name],
            capture_output=True,
            text=True,
            timeout=1800,  # 30 minute timeout
        )

        end_time = time.time()
        duration = end_time - start_time

        print(result.stdout)

        if result.stderr:
            print("STDERR:", result.stderr)

        success = result.returncode == 0

        if success:
            print(f"\n‚úÖ Test completed successfully in {duration:.1f}s")
        else:
            print(f"\n‚ùå Test failed with return code {result.returncode}")

        return {
            "script": script_name,
            "description": description,
            "success": success,
            "duration_seconds": round(duration, 2),
            "return_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
        }

    except subprocess.TimeoutExpired:
        end_time = time.time()
        duration = end_time - start_time

        print(f"\n‚è±Ô∏è  Test timed out after {duration:.1f}s")

        return {
            "script": script_name,
            "description": description,
            "success": False,
            "duration_seconds": round(duration, 2),
            "error": "Timeout",
        }

    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time

        print(f"\n‚ùå Error running test: {e}")

        return {
            "script": script_name,
            "description": description,
            "success": False,
            "duration_seconds": round(duration, 2),
            "error": str(e),
        }


def load_test_results():
    """Load results from all test output files."""
    results = {}

    result_files = {
        "model_comparison": "model_comparison_results.json",
        "rag_pipeline": "rag_test_results.json",
        "stress_test": "stress_test_results.json",
        "safety_triage": "safety_triage_results.json",
    }

    for test_name, filename in result_files.items():
        filepath = Path(filename)
        if filepath.exists():
            try:
                with open(filepath, "r") as f:
                    results[test_name] = json.load(f)
                print(f"‚úì Loaded {filename}")
            except Exception as e:
                print(f"‚úó Failed to load {filename}: {e}")
                results[test_name] = None
        else:
            print(f"‚úó File not found: {filename}")
            results[test_name] = None

    return results


def generate_master_report(execution_results, test_results):
    """Generate comprehensive master report."""
    print(f"\n\n{'=' * 80}")
    print("MASTER TEST SUITE REPORT")
    print(f"{'=' * 80}\n")

    print(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total Tests: {len(execution_results)}")

    # Execution summary
    print(f"\n{'Test Execution Summary':-^80}\n")
    print(f"{'Test':<35} {'Status':<10} {'Duration':<15} {'Return Code'}")
    print("-" * 80)

    total_duration = 0
    passed_count = 0

    for result in execution_results:
        status = "‚úÖ PASS" if result["success"] else "‚ùå FAIL"
        duration_str = f"{result['duration_seconds']:.1f}s"
        return_code = result.get("return_code", "N/A")

        print(
            f"{result['description']:<35} {status:<10} {duration_str:<15} {return_code}"
        )

        total_duration += result["duration_seconds"]
        if result["success"]:
            passed_count += 1

    print("-" * 80)
    print(
        f"{'Total':<35} {passed_count}/{len(execution_results)} {total_duration:.1f}s"
    )

    # Test results analysis
    print(f"\n\n{'Test Results Analysis':-^80}\n")

    # Model comparison
    if test_results.get("model_comparison"):
        print(f"\n{'1. Model Comparison':-^80}")
        mc = test_results["model_comparison"]

        if "results" in mc:
            for model, data in mc["results"].items():
                avg_latency = data["avg_latency_ms"]
                avg_tokens = data["avg_tokens"]
                high_risk_count = data["high_hallucination_risk_count"]

                print(f"\n{model}:")
                print(f"  Avg latency: {avg_latency:.0f}ms")
                print(f"  Avg tokens: {avg_tokens:.1f}")
                print(f"  High risk responses: {high_risk_count}/6")

    # RAG pipeline
    if test_results.get("rag_pipeline"):
        print(f"\n{'2. RAG Pipeline Coherence':-^80}")
        rag = test_results["rag_pipeline"]

        if "results" in rag:
            results = rag["results"]
            good_count = sum(1 for r in results if r["evaluation"]["quality"] == "GOOD")

            avg_retrieval = sum(r["retrieval_time_ms"] for r in results) / len(results)
            avg_generation = sum(r["generation_time_ms"] for r in results) / len(
                results
            )
            avg_total = sum(r["total_time_ms"] for r in results) / len(results)

            print(f"\nTests: {len(results)}")
            print(
                f"Quality GOOD: {good_count}/{len(results)} ({good_count / len(results) * 100:.0f}%)"
            )
            print(f"Avg retrieval time: {avg_retrieval:.0f}ms")
            print(f"Avg generation time: {avg_generation:.0f}ms")
            print(f"Avg total time: {avg_total:.0f}ms")

    # Stress test
    if test_results.get("stress_test"):
        print(f"\n{'3. Latency Stress Test':-^80}")
        stress = test_results["stress_test"]

        if "results" in stress:
            for result in stress["results"]:
                model = result["model"]
                success_rate = 100 - result["error_rate_percent"]
                p95 = result["latency_ms"]["p95"]
                actual_qps = result["actual_qps"]

                print(f"\n{model}:")
                print(f"  Success rate: {success_rate:.1f}%")
                print(f"  p95 latency: {p95:.0f}ms")
                print(f"  Actual QPS: {actual_qps:.2f}")

    # Safety triage
    if test_results.get("safety_triage"):
        print(f"\n{'4. Safety Triage':-^80}")
        safety = test_results["safety_triage"]

        if "results" in safety:
            for model, results in safety["results"].items():
                successful = [r for r in results if r.get("success", False)]

                if successful:
                    safe_count = sum(
                        1
                        for r in successful
                        if r["safety_analysis"]["safety_rating"]
                        in ["SAFE", "ACCEPTABLE"]
                    )
                    safe_pct = safe_count / len(successful) * 100

                    avg_score = sum(
                        r["safety_analysis"]["score"] for r in successful
                    ) / len(successful)

                    fabrication_count = sum(
                        1 for r in successful if r["safety_analysis"]["has_fabrication"]
                    )

                    print(f"\n{model}:")
                    print(
                        f"  Safe responses: {safe_count}/{len(successful)} ({safe_pct:.1f}%)"
                    )
                    print(f"  Avg safety score: {avg_score:.1f}/100")
                    print(f"  Fabrications: {fabrication_count}")

    # Recommendations
    print(f"\n\n{'Recommendations':-^80}\n")

    recommendations = []

    # Check stress test results
    if test_results.get("stress_test"):
        for result in test_results["stress_test"].get("results", []):
            if result["error_rate_percent"] > 5.0:
                recommendations.append(
                    f"‚ö†Ô∏è  {result['model']}: Error rate {result['error_rate_percent']:.1f}% exceeds 5% threshold. "
                    f"Consider reducing target QPS or investigating errors."
                )

            target_p95 = 12000 if "phi3" in result["model"] else 8000
            if result["latency_ms"]["p95"] > target_p95:
                recommendations.append(
                    f"‚ö†Ô∏è  {result['model']}: p95 latency {result['latency_ms']['p95']:.0f}ms exceeds target {target_p95}ms. "
                    f"May need hardware upgrade or reduced load."
                )

    # Check safety results
    if test_results.get("safety_triage"):
        for model, results in test_results["safety_triage"].get("results", {}).items():
            successful = [r for r in results if r.get("success", False)]

            if successful:
                fabrication_count = sum(
                    1 for r in successful if r["safety_analysis"]["has_fabrication"]
                )

                if fabrication_count > 3:
                    recommendations.append(
                        f"‚ö†Ô∏è  {model}: {fabrication_count} fabrications detected. "
                        f"Consider implementing stronger safety filters or using different model."
                    )

                safe_count = sum(
                    1
                    for r in successful
                    if r["safety_analysis"]["safety_rating"] in ["SAFE", "ACCEPTABLE"]
                )
                safe_pct = safe_count / len(successful) * 100

                if safe_pct < 70:
                    recommendations.append(
                        f"‚ö†Ô∏è  {model}: Only {safe_pct:.1f}% safe responses. "
                        f"Review prompt engineering and safety guardrails."
                    )

    if recommendations:
        for rec in recommendations:
            print(rec)
    else:
        print("‚úÖ All tests passed without major concerns!")

    # Overall assessment
    print(f"\n\n{'Overall Assessment':-^80}\n")

    if passed_count == len(execution_results):
        print("‚úÖ ALL TEST SUITES PASSED")
        print(
            "\nThe routing system is ready for production with the following confidence:"
        )
        print("  - Model selection logic validated")
        print("  - RAG pipeline coherence verified")
        print("  - Performance under load tested")
        print("  - Safety guardrails evaluated")
    else:
        print(f"‚ö†Ô∏è  {len(execution_results) - passed_count} TEST SUITE(S) FAILED")
        print("\nReview failed tests before production deployment.")


async def main():
    """Run all test suites."""
    print("\n" + "=" * 80)
    print("MASTER TEST SUITE RUNNER")
    print("Local LLM Routing Validation")
    print("=" * 80)
    print(f"\nStarting at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nThis will run 4 comprehensive test suites:")
    print("  1. Model comparison (tokenization, hallucination, quality)")
    print("  2. RAG pipeline coherence (qwen retrieval ‚Üí mistral synthesis)")
    print("  3. Latency stress test (production load simulation)")
    print("  4. Safety triage (ambiguous/risky prompts)")
    print("\nEstimated total time: 15-20 minutes")

    input("\nPress Enter to start...")

    # Test suite configurations
    tests = [
        {
            "script": "test_model_comparison.py",
            "description": "Model Comparison & Hallucination",
        },
        {"script": "test_rag_pipeline.py", "description": "RAG Pipeline Coherence"},
        {"script": "test_latency_stress.py", "description": "Latency Stress Test"},
        {"script": "test_safety_triage.py", "description": "Safety Triage"},
    ]

    execution_results = []

    # Run each test
    for i, test in enumerate(tests, 1):
        print(f"\n\n{'#' * 80}")
        print(f"TEST SUITE {i}/{len(tests)}")
        print(f"{'#' * 80}")

        result = run_test_script(test["script"], test["description"])
        execution_results.append(result)

        # Pause between tests (except after last one)
        if i < len(tests):
            print("\n‚è∏Ô∏è  Pausing 10s before next test suite...")
            await asyncio.sleep(10)

    # Load all test results
    print(f"\n\n{'=' * 80}")
    print("LOADING TEST RESULTS")
    print(f"{'=' * 80}\n")

    test_results = load_test_results()

    # Generate master report
    generate_master_report(execution_results, test_results)

    # Save master results
    master_output = {
        "test_date": datetime.now().isoformat(),
        "execution_results": execution_results,
        "test_results": test_results,
    }

    with open("master_test_results.json", "w") as f:
        json.dump(master_output, f, indent=2)

    print(f"\n\nüìä Master results saved to: master_test_results.json")
    print(f"\nCompleted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")


if __name__ == "__main__":
    asyncio.run(main())
